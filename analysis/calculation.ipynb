{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import git\n",
    "import pdb\n",
    "\n",
    "git_repo = git.Repo(\".\", search_parent_directories=True)\n",
    "git_root = git_repo.git.rev_parse(\"--show-toplevel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Percent Counts by Region Type from Annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d7dbfcc5f8444afa2395899012d9092",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/82 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# List of annotatated TSV files produced by annotator\n",
    "spidr_dir = os.path.join(git_root, \"annotated/uncompressed/spidr-annotated\")\n",
    "encode_dir = os.path.join(git_root, \"annotated/uncompressed/encode-annotated\")\n",
    "spidr_outputs = [os.path.join(spidr_dir, file) for file in os.listdir(spidr_dir)]\n",
    "encode_outputs = [os.path.join(encode_dir, file) for file in os.listdir(encode_dir)]\n",
    "all_outputs = spidr_outputs + encode_outputs\n",
    "\n",
    "# Column names based on annotator GitHub\n",
    "columns = [\"chromosome\", \"start\", \"stop\", \"name\", \"intensity\", \"strand\", \"gene_id\", \"gene_name\", \"genic_region_type\", \"all_overlapping_annotation\"]\n",
    "\n",
    "# List to store percent_by_type dataframes\n",
    "percent_by_count = []\n",
    "percent_by_sum = []\n",
    "raw_sum = []\n",
    "raw_count = []\n",
    "\n",
    "for file in tqdm(all_outputs, total=len(all_outputs)):\n",
    "    # Read in each file as a dataframe\n",
    "    df = pd.read_csv(file, sep=\"\\t\")\n",
    "    df.columns = columns\n",
    "    basename = os.path.basename(file).replace('.txt', '')\n",
    "    subset = df[[\"genic_region_type\", \"intensity\"]]\n",
    "\n",
    "    # Count by each region type and turn that into a percentage\n",
    "    sum_intensities = subset.groupby(by=[\"genic_region_type\"]).sum('intensity')\n",
    "    sum_intensities.columns = [f\"{basename}\"]\n",
    "    raw_sum.append(sum_intensities)\n",
    "    \n",
    "    # Count by each region type and turn that into a percentage\n",
    "    count_intensities = subset.groupby(by=[\"genic_region_type\"]).count()\n",
    "    count_intensities.columns = [f\"{basename}\"]\n",
    "    raw_count.append(count_intensities)\n",
    "    \n",
    "    # Getting percentages based on counts\n",
    "    total = count_intensities.sum().values.item()\n",
    "    percent = (count_intensities / total) * 100\n",
    "    percent_by_count.append(percent)\n",
    "    \n",
    "    # Getting percentages based on sums\n",
    "    total = sum_intensities.sum().values.item()\n",
    "    percent = (sum_intensities / total) * 100\n",
    "    percent_by_sum.append(percent)\n",
    "\n",
    "raw_count_df = pd.concat(raw_count, axis=1, join='outer')\n",
    "raw_count_df.fillna(value=0, inplace=True)\n",
    "\n",
    "raw_sum_df = pd.concat(raw_sum, axis=1, join='outer')\n",
    "raw_sum_df.fillna(value=0, inplace=True)\n",
    "\n",
    "percent_by_count_df = pd.concat(percent_by_count, axis=1, join='outer')\n",
    "percent_by_count_df.fillna(value=0, inplace=True)\n",
    "\n",
    "percent_by_sum_df = pd.concat(percent_by_sum, axis=1, join='outer')\n",
    "percent_by_sum_df.fillna(value=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep Bethyl and CST separately\n",
    "def get_filtered_cols(df):\n",
    "    tmp = df.copy()\n",
    "    new_tmp_cols = []\n",
    "    for col in tmp.columns:\n",
    "        if \"Bethyl\" in col or \"CST\" in col:\n",
    "            new_tmp_cols.append(\"_\".join(col.split(\"_\")[0:2]))\n",
    "        else:\n",
    "            new_tmp_cols.append(col.split(\"_\")[0])\n",
    "\n",
    "    tmp.columns = new_tmp_cols\n",
    "    return new_tmp_cols, tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentages by Summing Intensities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "spidr = percent_by_sum_df[sorted([col for col in percent_by_sum_df.columns.tolist() if \"_spidr_\" in col])]\n",
    "encode = percent_by_sum_df[sorted([col for col in percent_by_sum_df.columns.tolist() if \"_encode_\" in col])]\n",
    "\n",
    "new_spidr_cols, spidr = get_filtered_cols(spidr)\n",
    "new_encode_cols, encode = get_filtered_cols(encode)\n",
    "\n",
    "common_cols = [col for col in new_encode_cols if col in new_spidr_cols]\n",
    "encode_percentage_by_int_sum = encode[common_cols]\n",
    "spidr_percentage_by_int_sum = spidr[common_cols]\n",
    "\n",
    "spidr_percentage_by_int_sum.to_csv(os.path.join(git_root, \"output\", \"spidr_percent_by_intensity_sum.csv\"))\n",
    "encode_percentage_by_int_sum.to_csv(os.path.join(git_root, \"output\", \"encode_percent_by_intensity_sum.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percentage by Counting Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "spidr = percent_by_count_df[sorted([col for col in percent_by_count_df.columns.tolist() if \"_spidr_\" in col])]\n",
    "encode = percent_by_count_df[sorted([col for col in percent_by_count_df.columns.tolist() if \"_encode_\" in col])]\n",
    "\n",
    "new_spidr_cols, spidr = get_filtered_cols(spidr)\n",
    "new_encode_cols, encode = get_filtered_cols(encode)\n",
    "\n",
    "common_cols = [col for col in new_encode_cols if col in new_spidr_cols]\n",
    "raw_encode_percentage_by_row_count = encode[common_cols]\n",
    "raw_spidr_percentage_by_row_count = spidr[common_cols]\n",
    "\n",
    "raw_encode_percentage_by_row_count.to_csv(os.path.join(git_root, \"output\", \"encode_percent_by_row_count.csv\"))\n",
    "raw_spidr_percentage_by_row_count.to_csv(os.path.join(git_root, \"output\", \"spidr_percent_by_row_count.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Percent Counts by Region Type from Original Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the excel file, skipping the first row to ensure proper column names\n",
    "encode_supp_path = os.path.join(git_root, \"annotated/Summary_info_encode_Suppl_Data_4.xlsx\")\n",
    "encode_supp_data = pd.read_excel(encode_supp_path, skiprows=1)\n",
    "\n",
    "# Filtering for only 'K562' cell lines\n",
    "encode_supp_data = encode_supp_data[encode_supp_data['Cell Line'] == 'K562']\n",
    "\n",
    "# Get the list of gene symbols\n",
    "gene_symb = encode_supp_data[['Official Gene Symbol']]\n",
    "\n",
    "# Get all columns corresponding to total counts and counts of subsets (e.g. CDS, miRNA, etc)\n",
    "region_counts = encode_supp_data[encode_supp_data.columns[-17:].tolist()]\n",
    "\n",
    "# Merge the dataframes by index\n",
    "raw_counts = gene_symb.join(region_counts)\n",
    "raw_counts.set_index('Official Gene Symbol', inplace=True)\n",
    "raw_counts.to_csv(os.path.join(git_root, \"output\", \"encode_supp_raw_counts_by_region.csv\"))\n",
    "\n",
    "# Divide subset counts by total\n",
    "percent_counts = raw_counts[raw_counts.columns[1:]].div(raw_counts['IDR peak #'], axis=0) * 100\n",
    "\n",
    "# Transpose the dataframe so it's in the same shape as encode and spidr along with corresponding columns\n",
    "encode_supp = percent_counts.T[common_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "spidr = spidr_percentage_by_int_sum\n",
    "encode = encode_percentage_by_int_sum\n",
    "\n",
    "# Get indices for all 3 dataframes\n",
    "encode_supp_index = set(encode_supp.index.tolist())\n",
    "encode_index = set(encode.index.tolist())\n",
    "spidr_index = set(spidr.index.tolist())\n",
    "\n",
    "# Find region names shared by all 3 dataframes\n",
    "# common_idx = encode_supp_index.intersection(encode_index).intersection(spidr_index)\n",
    "common_idx = encode_index.intersection(spidr_index)\n",
    "\n",
    "# Keep only region names that are common to all dataframes\n",
    "# encode_supp = encode_supp.filter(items=common_idx, axis=0)\n",
    "encode = encode.filter(items=common_idx, axis=0)\n",
    "spidr = spidr.filter(items=common_idx, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicated columns by name (as opposed to by matching values)\n",
    "# encode_supp = encode_supp.loc[:, ~encode_supp.columns.duplicated()]\n",
    "encode = encode.loc[:, ~encode.columns.duplicated()]\n",
    "spidr = spidr.loc[:, ~spidr.columns.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_root = os.path.join(git_root, \"output\")\n",
    "\n",
    "# encode_supp.to_csv(os.path.join(output_root, \"encode_supp_percent_by_region.csv\"))\n",
    "encode.to_csv(os.path.join(output_root, \"encode_percent_by_region.csv\"))\n",
    "spidr.to_csv(os.path.join(output_root, \"spidr_percent_by_region.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spidr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b84a8ea01748ed9962056b8efb4fa6eeeab2887124ebb98eda71939fb16ed2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
