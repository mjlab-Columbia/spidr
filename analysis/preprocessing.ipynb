{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import git\n",
    "import pdb\n",
    "import sys\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "git_repo = git.Repo(\".\", search_parent_directories=True)\n",
    "git_root = git_repo.git.rev_parse(\"--show-toplevel\")\n",
    "uncompressed = os.path.join(git_root, \"annotated/uncompressed/\")\n",
    "\n",
    "# Column names based on annotator GitHub\n",
    "columns = [\"chromosome\", \"start\", \"stop\", \"name\", \"intensity\", \"strand\", \"gene_id\", \"gene_name\", \"genic_region_type\", \"all_overlapping_annotation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = {\n",
    "    'spidr': {\n",
    "        # 'peaks_switched_strands_filtered': os.path.join(uncompressed, \"spidr/peaks_switched_strands_filtered\"),\n",
    "        # 'miRNAadj': os.path.join(uncompressed, \"spidr/spidr_annotated_bed_with_miRNAadj\"),\n",
    "        'miRNAadj': os.path.join(uncompressed, \"spidr/spidr_annotated_bed_with_miRNAadj\")\n",
    "    },\n",
    "    'encode': {\n",
    "        'miRNAadj': os.path.join(uncompressed, \"encode/encode_annotated_bed_with_miRNAadj\"),\n",
    "        'downsampled_miRNAadj': os.path.join(uncompressed, \"encode/downsampled_encode_annotated_bed_with_miRNAadj\"),\n",
    "        # 'peaks_filtered': os.path.join(uncompressed, \"encode/peaks_filtered\"),\n",
    "        # 'downsampled_peaks_filtered': os.path.join(uncompressed, \"encode/downsampled_peaks_filtered\"),\n",
    "    }\n",
    "}\n",
    "\n",
    "# TODO: Figure out how to use traverse here\n",
    "files = {}\n",
    "for data_type in dirs.keys():\n",
    "    files[data_type] = {}\n",
    "    for key in dirs[data_type].keys():\n",
    "        dir = dirs[data_type][key]\n",
    "        files[data_type][key] = [os.path.join(dir, file) for file in os.listdir(dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_percentages(annotated_files, cols, percent_by='sum'):\n",
    "    percents = []\n",
    "    \n",
    "    for file in tqdm(annotated_files, total=len(annotated_files)):\n",
    "        # Check if file is empty\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                if len(f.read()) == 0:\n",
    "                    continue\n",
    "        except UnicodeDecodeError:\n",
    "            print(file)\n",
    "            sys.exit(0)\n",
    "\n",
    "        # Read in each file as a dataframe\n",
    "        df = pd.read_csv(file, sep=\"\\t\", names=cols)\n",
    "        basename = os.path.basename(file).replace('.txt', '')\n",
    "        subset = df[[\"genic_region_type\", \"intensity\"]]\n",
    "\n",
    "        if percent_by == 'sum':\n",
    "            intensities = subset.groupby(by=[\"genic_region_type\"]).sum('intensity')\n",
    "        elif percent_by == 'count':\n",
    "            intensities = subset.groupby(by=[\"genic_region_type\"]).count()\n",
    "\n",
    "        intensities.columns = [f\"{basename}\"]\n",
    "        total = intensities.sum().values.item()\n",
    "        percent_col = (intensities / total) * 100\n",
    "        percents.append(percent_col)\n",
    "\n",
    "\n",
    "    percents_df = pd.concat(percents, axis=1, join='outer')\n",
    "    percents_df.fillna(value=0, inplace=True)\n",
    "\n",
    "    return percents_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    tmp = df.copy()\n",
    "    new_tmp_cols = []\n",
    "\n",
    "    for col in tmp.columns:\n",
    "        # Keep Bethyl and CST separately\n",
    "        if \"Bethyl\" in col or \"CST\" in col:\n",
    "            new_tmp_cols.append(\"_\".join(col.split(\"_\")[0:2]))\n",
    "        else:\n",
    "            new_col = col.split(\"_\")[0]\n",
    "            if \"rep1\" in new_col:\n",
    "                new_col = new_col.replace(\"rep1\", \"\")\n",
    "            new_tmp_cols.append(new_col)\n",
    "\n",
    "    tmp.columns = new_tmp_cols\n",
    "    return new_tmp_cols, tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_percentage_tables(dict):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c1bc3e3cab74311851b4bb1306ae75f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/58 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a753cc5368a4ae0a3935d5f0aa70342",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d291f8e842b4b5cb4aa9c427abbeae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "spidr_df = get_percentages(files['spidr']['miRNAadj'], cols=columns, percent_by='sum')\n",
    "spidr_cols, spidr_df = rename_columns(spidr_df)\n",
    "\n",
    "encode_df = get_percentages(files['encode']['miRNAadj'], cols=columns, percent_by='sum')\n",
    "encode_cols, encode_df = rename_columns(encode_df)\n",
    "\n",
    "encode_downsampled_df = get_percentages(files['encode']['downsampled_miRNAadj'], cols=columns, percent_by='sum')\n",
    "encode_downsampled_cols, encode_downsampled_df = rename_columns(encode_downsampled_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode Percent Counts by Region Type from Original Paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the excel file, skipping the first row to ensure proper column names\n",
    "encode_supp_path = os.path.join(git_root, \"annotated/Summary_info_encode_Suppl_Data_4.xlsx\")\n",
    "encode_supp_data = pd.read_excel(encode_supp_path, skiprows=1)\n",
    "\n",
    "# Filtering for only 'K562' cell lines\n",
    "encode_supp_data = encode_supp_data[encode_supp_data['Cell Line'] == 'K562']\n",
    "\n",
    "# Get the list of gene symbols\n",
    "gene_symb = encode_supp_data[['Official Gene Symbol']]\n",
    "\n",
    "# Get all columns corresponding to total counts and counts of subsets (e.g. CDS, miRNA, etc)\n",
    "region_counts = encode_supp_data[encode_supp_data.columns[-17:].tolist()]\n",
    "\n",
    "# Merge the dataframes by index\n",
    "raw_counts = gene_symb.join(region_counts)\n",
    "raw_counts.set_index('Official Gene Symbol', inplace=True)\n",
    "raw_counts.to_csv(os.path.join(git_root, \"output\", \"encode_supp_raw_counts_by_region.csv\"))\n",
    "\n",
    "# Divide subset counts by total\n",
    "percent_counts = raw_counts[raw_counts.columns[1:]].div(raw_counts['IDR peak #'], axis=0) * 100\n",
    "\n",
    "# Transpose the dataframe so it's in the same shape as encode and spidr along with corresponding columns\n",
    "encode_supp = percent_counts.T\n",
    "\n",
    "# Manually renaming things in supplementary data to match annotator output\n",
    "supp_to_annot = {\n",
    "    \"distintron\": \"distintron500\",\n",
    "    \"noncoding_distintron\": \"distnoncoding_intron500\",\n",
    "    \"proxintron\": \"proxintron500\",\n",
    "    \"noncoding_proxintron\": \"proxnoncoding_intron500\",\n",
    "    # \"miRNA_proximal\": \"miRNA_adjacent\"\n",
    "}\n",
    "\n",
    "new_index = {}\n",
    "\n",
    "for idx in encode_supp.index:\n",
    "    if idx in supp_to_annot:\n",
    "        new_index[idx] = supp_to_annot[idx]\n",
    "    else:\n",
    "        new_index[idx] = idx\n",
    "\n",
    "encode_tmp = encode_supp.copy()\n",
    "\n",
    "encode_tmp = encode_tmp.rename(index=new_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_dfs(ref, dfs):\n",
    "    # Find all the columns with antibody variants in the reference dataframe\n",
    "    cols = [col for col in ref.columns if (\"CST\" in col) or (\"Bethyl\" in col)]\n",
    "    \n",
    "    for col in cols:\n",
    "        for df in dfs:\n",
    "            # If antibody variant isn't present, add it as a column\n",
    "            if col not in df.columns:\n",
    "                protein_name = col.split(\"_\")[0]\n",
    "                antibody = col.split(\"_\")[1]\n",
    "                df[f'{protein_name}_{antibody}'] = df[f'{protein_name}']\n",
    "\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_df_aug, encode_downsampled_df_aug, encode_supp_aug = augment_dfs(spidr_df, [encode_df, encode_downsampled_df, encode_tmp])\n",
    "\n",
    "dfs = {\n",
    "    \"spidr\": spidr_df,\n",
    "    \"encode\": encode_df_aug,\n",
    "    \"encode_downsampled\": encode_downsampled_df_aug,\n",
    "    \"encode_supp\": encode_supp_aug\n",
    "}\n",
    "\n",
    "final = {key: None for key in dfs.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_common(dataframe_dict):\n",
    "    cols = []\n",
    "    indices = []\n",
    "    \n",
    "    for df in dataframe_dict.values():\n",
    "        cols.append(set(df.columns))\n",
    "        indices.append(set(df.index))\n",
    "    \n",
    "    common_cols = list(cols[0].intersection(*cols[1:]))\n",
    "    common_indices = list(indices[0].intersection(*indices[1:]))\n",
    "    return common_cols, common_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_cols, common_idx = get_common(dfs)\n",
    "output_root = os.path.join(git_root, \"output\")\n",
    "\n",
    "for key, df in dfs.items():\n",
    "    # Keep common columns\n",
    "    tmp = df.copy()\n",
    "    tmp = tmp[common_cols]\n",
    "\n",
    "    # Keep only region names that are common to all dataframes\n",
    "    tmp = tmp.filter(items=common_idx, axis=0)\n",
    "    \n",
    "    # Remove duplicated columns\n",
    "    tmp = tmp.loc[:, ~tmp.columns.duplicated()]\n",
    "\n",
    "    # Save to disk\n",
    "    final[key] = tmp\n",
    "    final[key].to_csv(os.path.join(output_root, f\"{key}_percent_by_region.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('spidr')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b84a8ea01748ed9962056b8efb4fa6eeeab2887124ebb98eda71939fb16ed2ef"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
