'''
Aim: A Snakemake workflow to process SPIDR BAMs
'''

##############################################################################
#Initialize settings
##############################################################################

try:
    config_path = config["config_path"]
except:
    config_path = 'config.postprocess-targets.yaml'

configfile: config_path

try:
    SAMPLE_STR = config['samples']
except:
    print('No sample names provided. No default')
    sys.exit()

try:
    out_dir = config['output_dir']
except:
    out_dir = ''

try:
    resolution = config['bin_size']
except:
    resolution = 1000

try:
    conditions = config['conditions']
except:
    print("No conditions provided. No default")
    sys.exit()

os.makedirs("workup/logs/cluster", exist_ok=True)
os.makedirs("benchmarks", exist_ok=True)
##############################################################################
# Location of Scripts
##############################################################################
random_downsample_clip = "scripts/java/RandomDownsample_CLIP.jar"

################################################################################
#Get sample files
###############################################################################

#Prep samples from fastq directory using fastq2json_updated.py, now load json file
exclude = ['ambiguous', 'uncertain', '', 'merged', 'none']
TARGETS = []

# Iterate through workup/splitbams and find all targets generated by generate-targets.smk pipeline
# Format of files is: <sample name>.<condition>.RPM_<target>.bam
for filename in os.listdir("workup/splitbams"):
    if filename.endswith('.bam'):
        filename = filename.strip('.bam')
        identifier = filename.split('.')[-1]
        target_string = identifier.split('_')

        # Helps exclude the case of no target (i.e. ...RPM.bam)
        if len(target_string) == 2:
            target = target_string[-1]
            if target not in exclude:
                TARGETS.append(target)

SAMPLE_LIST = SAMPLE_STR.split(' ')

BAMS_BY_CONDITION = expand(
    out_dir + "workup/splitbams/{sample}.{condition}.RPM_{target}.bam", 
    sample=SAMPLE_LIST, 
    condition=conditions,
    target=TARGETS
)

BAMS_BY_CONDITION = expand(
    [   
        "workup/splitbams/{sample}.{condition}_{target}.bam",
        "workup/splitbams/{sample}_{target}.bam", 
    ],
    sample=SAMPLE_LIST, 
    condition=conditions,
    target=TARGETS
)

MERGED = expand(
    out_dir + "workup/mergedbams/{target}.merged.bam",
    target=TARGETS
)

BIGWIGS = expand(
    out_dir + "workup/bigwigs/{target}.{bs}.bigwig", 
    target=TARGETS, 
    bs=resolution
) 

BED_FILES = expand(
    [
        out_dir + "workup/downsample/{sample}.{target}.{condition}.actual.bedgraph",
        out_dir + "workup/downsample/{sample}.{target}.{condition}.diff.bedgraph",
        out_dir + "workup/downsample/{sample}.{target}.{condition}.pval.bed",
    ],
    sample=SAMPLE_LIST,
    condition=conditions,
    target=TARGETS
)

################################################################################
################################################################################
#RULE ALL
################################################################################
################################################################################

rule all:
    input: BAMS_BY_CONDITION + BIGWIGS + BED_FILES #+ MERGED 

#rule merge_bams:
#    input:
#        expand(
#            [   
#                "workup/splitbams/{sample}.{condition}_{{target}}.bam",
#                "workup/splitbams/{sample}_{{target}}.bam", 
#            ],
#            sample=SAMPLE_LIST, 
#            condition=conditions,
#        )
#    output:
#        "workup/mergedbams/{target}.merged.bam"
#    conda:
#        "envs/sprite.yaml"
#    log:
#        out_dir + "workup/logs/{target}.merge_bams.log"
#    benchmark:
#        out_dir + "workup/logs/{target}.merge_bams.tsv"
#    threads:
#        8
#    shell:
#        '''
#        samtools merge -@ {threads} {output} {input}
#        samtools index {output}
#        '''

rule make_bigwigs:
    input:
        "workup/mergedbams/{target}.merged.bam" 
    output:
        out_dir + "workup/bigwigs/{target}.{bs}.bigwig"
    conda:
        "envs_post/deeptools.yaml"
    log:
        out_dir + "workup/logs/{target}.{bs}.make_bigwigs.log"
    benchmark:
        "benchmarks/{target}.{bs}.make_bigwigs.tsv"
    shell:
        '''
        bamCoverage -p max -b {input} -o {output} -bs {resolution} -of bigwig >& {log}
        '''

rule random_downsample_clip:
    input:
        out_dir + "workup/splitbams/{sample}.{condition}_{target}.bam"
    output: 
        actual = out_dir + "workup/downsample/{sample}.{target}.{condition}.actual.bedgraph",
        diff = out_dir + "workup/downsample/{sample}.{target}.{condition}.diff.bedgraph",
        pval = out_dir + "workup/downsample/{sample}.{target}.{condition}.pval.bed"
    params:
        control_bams = lambda wildcards: " ".join(
            expand(
                out_dir + "workup/splitbams/{sample}.{condition}_{target}.bam",
                sample=wildcards.sample,
                condition=conditions,
                target=[t for t in TARGETS if t != wildcards.target])
            ),
        prefix = out_dir + "workup/downsample/{sample}.{target}.{condition}"
    conda:
        "envs/java.yaml"
    log: 
        "workup/logs/{sample}_{target}_{condition}.randomdownsampleclip.log"
    shell:
        """  
        (java \
            -jar \
            --enable-preview \
            {random_downsample_clip} \
            {input} \
            {params.control_bams} \
            {params.prefix}) &> {log}
        """ 

